to start in NLP natrual language processing 

step 1: clean the texts

and this can be done using nltk library

to download form Nltk

import nltk
nltk.download('popular')

to download only stopwords 
#nltk.download('stopwords')

nltk used for alot of thing like stopwords,getting the root of the verb

case of use:

from nltk.corpus import stopwords	#for stopwords
from nltk.stem.porter import PorterStemmer	# for the root of verb


ps = PorterStemmer()
review = [ps.stem(word) for word in review.split() if not word in set(stopwords.words('english'))]

corpus is a collection of text

step 2: Creating the Bag of Words model

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features=1500)
X = cv.fit_transform(corpus).toarray() 
# will create sparse matrix
# putting all the different words in its own column

step 3: select a model 
the common model for NLP is naive Bayes, Decision Tree, Random Forest




If you are up for some practical activities, here is a little challenge:

1. Run the other classification models we made in Part 3 - Classification, other than the one we used in the last tutorial.

2. Evaluate the performance of each of these models. Try to beat the Accuracy obtained in the tutorial. But remember, Accuracy is not enough, so you should also look at other performance metrics like Precision (measuring exactness), Recall (measuring completeness) and the F1 Score (compromise between Precision and Recall). Please find below these metrics formulas (TP = # True Positives, TN = # True Negatives, FP = # False Positives, FN = # False Negatives):

Accuracy = (TP + TN) / (TP + TN + FP + FN)

Precision = TP / (TP + FP)

Recall = TP / (TP + FN)

F1 Score = 2 * Precision * Recall / (Precision + Recall)

3. Try even other classification models that we haven't covered in Part 3 - Classification. Good ones for NLP include:

    CART
    C5.0
    Maximum Entropy

Submit your results in the Q&A for this Lecture or by pm and justify in few words why you think it's the most appropriate model.
